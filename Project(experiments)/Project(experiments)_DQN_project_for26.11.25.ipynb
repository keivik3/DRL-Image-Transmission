{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Yqj2Dst_AIF9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — imports & hyperparams\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# DQN / env hyperparams\n",
    "CODE_DIM = 128          # must match encoder code_dim\n",
    "N_PREV = 5              # length of context used by decoder\n",
    "K_CHOICES = [128, 96, 64, 32]   # actions = choose K (absolute #latent dims)\n",
    "LAMBDA = 0.01           # penalty on K in reward\n",
    "MAX_EPISODES = 300\n",
    "MAX_STEPS_PER_EP = 400\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "LR_DQN = 1e-3\n",
    "REPLAY_CAP = 10000\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 2000\n",
    "TAU = 0.995   # soft update factor for target network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zIJOnpEDAdOF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: encoder.pth not found. Run AE training first.\n",
      "Warning: decoder.pth not found. Run AE training first.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — load encoder/decoder classes and weights\n",
    "from encoder import Encoder   # expects encoder.py present\n",
    "from decoder import Decoder   # expects decoder.py present\n",
    "\n",
    "# instantiate and load saved weights (from AE training)\n",
    "encoder = Encoder(input_ch=1, code_dim=CODE_DIM).to(DEVICE)\n",
    "decoder = Decoder(code_dim=CODE_DIM, n_prev=N_PREV, output_ch=1).to(DEVICE)\n",
    "\n",
    "if os.path.exists(\"encoder.pth\"):\n",
    "    encoder.load_state_dict(torch.load(\"encoder.pth\", map_location=DEVICE))\n",
    "    print(\"Loaded encoder.pth\")\n",
    "else:\n",
    "    print(\"Warning: encoder.pth not found. Run AE training first.\")\n",
    "\n",
    "if os.path.exists(\"decoder.pth\"):\n",
    "    decoder.load_state_dict(torch.load(\"decoder.pth\", map_location=DEVICE))\n",
    "    print(\"Loaded decoder.pth\")\n",
    "else:\n",
    "    print(\"Warning: decoder.pth not found. Run AE training first.\")\n",
    "\n",
    "# freeze AE weights (we only train DQN)\n",
    "encoder.eval(); decoder.eval()\n",
    "for p in encoder.parameters(): p.requires_grad = False\n",
    "for p in decoder.parameters(): p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MU0zaVKyAdoO"
   },
   "outputs": [],
   "source": [
    "# Cell 3 — LengthEnv: uses encoder & decoder, returns observation vector and reward\n",
    "import gymnasium as gym\n",
    "from PIL import Image\n",
    "\n",
    "class LengthEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Action: Discrete index selecting K from K_CHOICES.\n",
    "    Observation: concatenation of cart state (4 dims) + first preview_dim elements of current z (float).\n",
    "    Reward: PSNR between reconstructed (using truncated z and previous truncated z buffer) and original frame\n",
    "            minus lambda * (K) penalty (we use K / CODE_DIM scaled or absolute depending on preference).\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, device, preview_dim=16, lambda_penalty=LAMBDA, max_steps=400):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.encoder = encoder.to(device)\n",
    "        self.decoder = decoder.to(device)\n",
    "        self.preview_dim = preview_dim\n",
    "        self.lambda_penalty = lambda_penalty\n",
    "        self.max_steps = max_steps\n",
    "        self.length_levels = K_CHOICES\n",
    "        self.action_space = gym.spaces.Discrete(len(self.length_levels))\n",
    "\n",
    "        # CartPole observation ranges (approx)\n",
    "        cart_low = np.array([-4.8, -np.finfo(np.float32).max, -0.418, -np.finfo(np.float32).max], dtype=np.float32)\n",
    "        cart_high = -cart_low\n",
    "        obs_low = np.concatenate([cart_low, np.full(self.preview_dim, -10.0)], axis=0)\n",
    "        obs_high = np.concatenate([cart_high, np.full(self.preview_dim, 10.0)], axis=0)\n",
    "        self.observation_space = gym.spaces.Box(low=obs_low, high=obs_high, dtype=np.float32)\n",
    "\n",
    "        # underlying environment for frames and dynamics\n",
    "        self.env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "        self.reset()\n",
    "\n",
    "    def _preprocess(self, frame):\n",
    "        # frame is RGB ndarray, convert to gray 64x64 normalized [0,1]\n",
    "        img = Image.fromarray(np.asarray(frame)).convert(\"L\").resize((64,64), Image.BILINEAR)\n",
    "        arr = np.asarray(img, dtype=np.float32) / 255.0\n",
    "        arr = arr[None, None, :, :]  # (1,1,H,W)\n",
    "        return torch.tensor(arr, device=self.device)\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        res = self.env.reset(seed=seed)\n",
    "        # gymnasium reset may return (obs, info) or obs only; handle both\n",
    "        if isinstance(res, tuple) and len(res) >= 1:\n",
    "            obs = res[0]\n",
    "        else:\n",
    "            obs = res\n",
    "        self.cart_state = np.array(obs, dtype=np.float32)\n",
    "        frame = self.env.render()\n",
    "        self.frame = self._preprocess(frame)\n",
    "        self.step_count = 0\n",
    "        self.buffer = deque(maxlen=N_PREV)\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # encode current frame to latent and expose preview_dim of it\n",
    "        with torch.no_grad():\n",
    "            z = encoder(self.frame) if callable(encoder) else encoder(self.frame)\n",
    "        preview = z[0, :self.preview_dim].cpu().numpy()\n",
    "        return np.concatenate([self.cart_state, preview], axis=0).astype(np.float32)\n",
    "\n",
    "    def step(self, action_idx):\n",
    "        K = self.length_levels[int(action_idx)]\n",
    "        # encode current frame\n",
    "        with torch.no_grad():\n",
    "            z = encoder(self.frame)  # shape (1, code_dim)\n",
    "        z_limited = z.clone()\n",
    "        if K < z.shape[1]:\n",
    "            z_limited[:, K:] = 0.0\n",
    "\n",
    "        # prepare prev_latents for decoder: if buffer less than N_PREV, use zeros\n",
    "        if len(self.buffer) < N_PREV:\n",
    "            prev = None\n",
    "        else:\n",
    "            prev = torch.cat(list(self.buffer), dim=0).unsqueeze(0).view(1, N_PREV, CODE_DIM).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rec = decoder.decode(z_limited, prev_latents=prev)  # (1,1,H,W)\n",
    "        mse = F.mse_loss(rec, self.frame).item()\n",
    "        psnr = 10.0 * math.log10(1.0 / (mse + 1e-8))\n",
    "        # penalty is proportional to K (absolute) scaled — choose whichever is meaningful\n",
    "        penalty = self.lambda_penalty * (K / float(CODE_DIM))\n",
    "        reward = psnr - penalty\n",
    "\n",
    "        # step the underlying env with a simple balancing policy for continuity (or random)\n",
    "        # here we use a simple heuristic controller for the cart to avoid quick terminations\n",
    "        action_env = 1 if self.cart_state[2] > 0 else 0\n",
    "        step_res = self.env.step(action_env)\n",
    "        # gymnasium step returns (obs, reward, terminated, truncated, info); handle variants\n",
    "        if len(step_res) == 5:\n",
    "            obs, _, terminated, truncated, info = step_res\n",
    "        else:\n",
    "            # legacy gym: obs, reward, done, info\n",
    "            obs, _, done, info = step_res\n",
    "            terminated = done; truncated = False\n",
    "\n",
    "        self.cart_state = np.array(obs, dtype=np.float32)\n",
    "        self.frame = self._preprocess(self.env.render())\n",
    "        self.step_count += 1\n",
    "        done = terminated or truncated or (self.step_count >= self.max_steps)\n",
    "        return self._get_obs(), float(reward), bool(done), False, {}\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iHR91yM1Afft"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Victoriya\\AppData\\Roaming\\Python\\Python312\\site-packages\\gymnasium\\spaces\\box.py:236: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current [... truncated ...]\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Initialized fc_enc with input size 4096\n",
      "Env observation dim: 20 n_actions: 4\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — DQN code + replay memory\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_obs, n_actions):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_obs, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "# instantiate environment\n",
    "env = LengthEnv(encoder, decoder, DEVICE, preview_dim=16, lambda_penalty=LAMBDA, max_steps=MAX_STEPS_PER_EP)\n",
    "n_actions = env.action_space.n\n",
    "n_obs = env.observation_space.shape[0]\n",
    "print(\"Env observation dim:\", n_obs, \"n_actions:\", n_actions)\n",
    "policy_net = DQN(n_obs, n_actions).to(DEVICE)\n",
    "target_net = DQN(n_obs, n_actions).to(DEVICE)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR_DQN)\n",
    "memory = ReplayMemory(REPLAY_CAP)\n",
    "steps_done = 0\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if random.random() > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            s = torch.tensor(state, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "            return int(policy_net(s).argmax(dim=1).item())\n",
    "    else:\n",
    "        return random.randrange(n_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UzPvNg4iAh6C"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Victoriya\\AppData\\Local\\Temp\\ipykernel_11040\\2838671163.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the lis[... truncated ...]\n",
      "  non_final_next = torch.tensor([s for s in batch.next_state if s is not None], dtype=torch.float32, device=DEVICE) if any(non_final_mask) else None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10/300 | Reward: 262.31 | avg50: 280.97\n",
      "Episode 20/300 | Reward: 262.34 | avg50: 287.53\n",
      "Episode 30/300 | Reward: 241.61 | avg50: 279.59\n",
      "Episode 40/300 | Reward: 269.20 | avg50: 272.86\n",
      "Episode 50/300 | Reward: 345.21 | avg50: 283.04\n",
      "Episode 60/300 | Reward: 358.92 | avg50: 288.71\n",
      "Episode 70/300 | Reward: 372.78 | avg50: 286.08\n",
      "Episode 80/300 | Reward: 324.42 | avg50: 289.11\n",
      "Episode 90/300 | Reward: 338.22 | avg50: 294.35\n",
      "Episode 100/300 | Reward: 379.73 | avg50: 286.49\n",
      "Episode 110/300 | Reward: 310.69 | avg50: 277.09\n",
      "Episode 120/300 | Reward: 331.39 | avg50: 282.48\n",
      "Episode 130/300 | Reward: 269.30 | avg50: 281.25\n",
      "Episode 140/300 | Reward: 407.25 | avg50: 284.29\n",
      "Episode 150/300 | Reward: 358.98 | avg50: 286.77\n",
      "Episode 160/300 | Reward: 379.65 | avg50: 293.81\n",
      "Episode 170/300 | Reward: 179.52 | avg50: 288.01\n",
      "Episode 180/300 | Reward: 296.90 | avg50: 291.03\n",
      "Episode 190/300 | Reward: 220.95 | avg50: 286.89\n",
      "Episode 200/300 | Reward: 296.87 | avg50: 284.96\n",
      "Episode 210/300 | Reward: 172.55 | avg50: 274.89\n",
      "Episode 220/300 | Reward: 214.05 | avg50: 271.43\n",
      "Episode 230/300 | Reward: 310.71 | avg50: 272.13\n",
      "Episode 240/300 | Reward: 276.15 | avg50: 278.63\n",
      "Episode 250/300 | Reward: 289.99 | avg50: 276.68\n",
      "Episode 260/300 | Reward: 352.08 | avg50: 283.03\n",
      "Episode 270/300 | Reward: 269.26 | avg50: 296.98\n",
      "Episode 280/300 | Reward: 283.10 | avg50: 299.89\n",
      "Episode 290/300 | Reward: 283.06 | avg50: 287.46\n",
      "Episode 300/300 | Reward: 310.67 | avg50: 293.41\n",
      "DQN training complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — optimization helper and training loop\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    state_batch = torch.tensor(np.stack(batch.state), dtype=torch.float32, device=DEVICE)\n",
    "    action_batch = torch.tensor(batch.action, dtype=torch.int64, device=DEVICE).unsqueeze(1)\n",
    "    reward_batch = torch.tensor(batch.reward, dtype=torch.float32, device=DEVICE).unsqueeze(1)\n",
    "\n",
    "    non_final_mask = torch.tensor([s is not None for s in batch.next_state], device=DEVICE, dtype=torch.bool)\n",
    "    non_final_next = torch.tensor([s for s in batch.next_state if s is not None], dtype=torch.float32, device=DEVICE) if any(non_final_mask) else None\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, 1, device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        if non_final_next is not None:\n",
    "            next_state_values[non_final_mask] = target_net(non_final_next).max(1)[0].unsqueeze(1)\n",
    "\n",
    "    expected_values = reward_batch + GAMMA * next_state_values\n",
    "\n",
    "    loss = nn.SmoothL1Loss()(state_action_values, expected_values.detach())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 5.0)\n",
    "    optimizer.step()\n",
    "\n",
    "# Training loop\n",
    "episode_rewards = []\n",
    "for ep in range(1, MAX_EPISODES + 1):\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0.0\n",
    "    for t in range(MAX_STEPS_PER_EP):\n",
    "        action = select_action(obs)\n",
    "        next_obs, reward, done, _, _ = env.step(action)\n",
    "        memory.push(obs, action, None if done else next_obs, reward)\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "\n",
    "        optimize_model()\n",
    "\n",
    "        # soft update target\n",
    "        for tp, pp in zip(target_net.parameters(), policy_net.parameters()):\n",
    "            tp.data.copy_(TAU * pp.data + (1.0 - TAU) * tp.data)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    episode_rewards.append(total_reward)\n",
    "    if ep % 10 == 0:\n",
    "        avg_last = np.mean(episode_rewards[-50:]) if len(episode_rewards) >= 1 else total_reward\n",
    "        print(f\"Episode {ep}/{MAX_EPISODES} | Reward: {total_reward:.2f} | avg50: {avg_last:.2f}\")\n",
    "\n",
    "print(\"DQN training complete.\")\n",
    "# save policy net\n",
    "torch.save(policy_net.state_dict(), \"dqn_policy.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rHuyEVADAjq7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total saved frames: 35\n",
      "FFMPEG write failed: TiffWriter.write() got an unexpected keyword argument 'fps'\n",
      "Saved dqn_reconstructed.gif (fallback).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video width=320 controls>\n",
       "                     <source src=\"dqn_reconstructed.mp4\" type=\"video/mp4\">\n",
       "                     Your browser does not support the video tag.\n",
       "                   </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAE8CAYAAACB0Lt0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA8xklEQVR4nO3d[...truncated...]",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 6 — evaluate greedy policy and save reconstructed video (fixed)\n",
    "import imageio\n",
    "import numpy as np\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "policy_net.eval()\n",
    "obs, _ = env.reset()\n",
    "frames_out = []\n",
    "chosen_Ks = []\n",
    "with torch.no_grad():\n",
    "    for t in range(1000):   # generate up to 1000 frames or until done\n",
    "        s = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "        act = int(policy_net(s).argmax(dim=1).item())  # greedy\n",
    "        chosen_Ks.append(K_CHOICES[act])\n",
    "\n",
    "        # step environment (this updates env.frame and env.buffer)\n",
    "        next_obs, reward, done, _, _ = env.step(act)\n",
    "\n",
    "        # reconstruct current frame using AE (same as env did)\n",
    "        with torch.no_grad():\n",
    "            z = encoder(env.frame)                     # (1, CODE_DIM)\n",
    "            K = K_CHOICES[act]\n",
    "            z_l = z.clone()\n",
    "            if K < z.shape[1]:\n",
    "                z_l[:, K:] = 0.0\n",
    "            prev = None if len(env.buffer) < N_PREV else torch.cat(list(env.buffer), dim=0).unsqueeze(0).view(1, N_PREV, CODE_DIM)\n",
    "            rec = decoder.decode(z_l, prev_latents=prev)  # (1,1,H,W)\n",
    "            frame_np = (rec.cpu().numpy()[0,0] * 255).astype(np.uint8)  # (H,W) uint8\n",
    "            # convert gray->RGB for ffmpeg-compatible frames\n",
    "            frame_rgb = np.stack([frame_np, frame_np, frame_np], axis=2)  # (H,W,3)\n",
    "            frames_out.append(frame_rgb)\n",
    "\n",
    "        obs = next_obs\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "print(\"Total saved frames:\", len(frames_out))\n",
    "# --- ensure ffmpeg plugin is available; try to save with imageio-ffmpeg backend\n",
    "try:\n",
    "    imageio.get_writer(\"dqn_reconstructed.mp4\", fps=30, codec='libx264').append_data  # quick check\n",
    "    with imageio.get_writer(\"dqn_reconstructed.mp4\", fps=30, codec='libx264') as writer:\n",
    "        for fr in frames_out:\n",
    "            writer.append_data(fr)\n",
    "    print(\"Saved dqn_reconstructed.mp4, frames:\", len(frames_out))\n",
    "except Exception as e:\n",
    "    print(\"FFMPEG write failed:\", e)\n",
    "    # fallback: save as GIF (slower & larger) — useful for debugging\n",
    "    try:\n",
    "        gif_frames = [(fr[...,0]).astype(np.uint8) for fr in frames_out]  # grayscale frames\n",
    "        imageio.mimsave(\"dqn_reconstructed.gif\", gif_frames, fps=10)\n",
    "        print(\"Saved dqn_reconstructed.gif (fallback).\")\n",
    "    except Exception as e2:\n",
    "        print(\"Fallback GIF also failed:\", e2)\n",
    "\n",
    "# display video in notebook (works in Colab/Jupyter)\n",
    "if os.path.exists(\"dqn_reconstructed.mp4\"):\n",
    "    display(HTML(f\"\"\"<video width=320 controls>\n",
    "                     <source src=\"dqn_reconstructed.mp4\" type=\"video/mp4\">\n",
    "                     Your browser does not support the video tag.\n",
    "                   </video>\"\"\"))\n",
    "elif os.path.exists(\"dqn_reconstructed.gif\"):\n",
    "    display(HTML(f'<img src=\"dqn_reconstructed.gif\" />'))\n",
    "\n",
    "# show histogram of chosen K\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.hist(chosen_Ks, bins=np.arange(len(K_CHOICES)+1)-0.5, rwidth=0.6)\n",
    "plt.xticks(range(len(K_CHOICES)), K_CHOICES)\n",
    "plt.xlabel(\"Chosen K\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of selected K under greedy policy\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xc-HO011AnDg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },

  // --------------------------- NEW CELLS ADDED BELOW ---------------------------
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppo-intro"
   },
   "source": [
    "## PPO: Add Proximal Policy Optimization (third algorithm)\n",
    "\n",
    "The following cells add a PPO baseline (stable-baselines3) so you can compare three algorithms: DQN (existing), A2C (if present in your notebook), and PPO.  \n",
    "- The code includes a light-weight SB3 install cell (run only if SB3 is not installed).  \n",
    "- I wrap your ImageTransmission environment into a simple Gym wrapper suitable for SB3 and use Monitor to log episode rewards/lengths.  \n",
    "- Default PPO hyperparameters are small (\"nodest\") so runtime is reasonable. Change the hyperparameters in the cells or at the top of the notebook to scale up experiments.  \n",
    "\n",
    "Where to change training size (quick reference):\n",
    "- PPO_TOTAL_TIMESTEPS: increase to 1e5 - 1e6 for serious runs\n",
    "- n_steps / batch_size / n_epochs: change per PPO best-practices\n",
    "\n",
    "Notes:\n",
    "- The eval cell later reconstructs the same images used by the DQN/A2C evaluation and saves reconstructions + MSEs for comparison.  \n",
    "- If you run this in Colab, use a GPU runtime for speed.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppo-install"
   },
   "outputs": [],
   "source": [
    "# Optional: install stable-baselines3 and sb3-contrib if not available in the environment\n",
    "# Uncomment and run this cell if SB3 is missing (Colab or fresh envs).\n",
    "# !pip install stable-baselines3 sb3-contrib[extra] --quiet\n",
    "print('Skip SB3 install if already available; uncomment cell to install in a fresh environment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppo-wrapper"
   },
   "outputs": [],
   "source": [
    "# Gym wrapper for ImageTransmissionEnv + Monitor setup for SB3\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "class GymImageTransmissionEnv(gym.Env):\n",
    "    \"\"\"Simple Gym wrapper for the ImageTransmissionEnv used by DQN/A2C above.\n",
    "    Observation: PREVIEW_DIM floats; Action: Discrete(len(K_CHOICES)).\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dataset, encoder, decoder, device,\n",
    "                 preview_dim=16, lambda_penalty=0.01, max_steps=100):\n",
    "        super().__init__()\n",
    "        self.inner = ImageTransmissionEnv(image_dataset, encoder, decoder, device,\n",
    "                                          preview_dim=preview_dim, lambda_penalty=lambda_penalty,\n",
    "                                          max_steps=max_steps)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(preview_dim,), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(len(K_CHOICES))\n",
    "    def reset(self, seed=None, options=None):\n",
    "        obs = self.inner.reset()\n",
    "        return obs, {}\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.inner.step(action)\n",
    "        # Return Gymnasium-compatible tuple (obs, reward, terminated, truncated, info)\n",
    "        return obs, reward, done, False, info\n",
    "    def close(self):\n",
    "        self.inner.close()\n",
    "\n",
    "# Create a monitored env for PPO training (Monitor logs per-episode rewards & lengths)\n",
    "os.makedirs('outputs/ppo_monitor', exist_ok=True)\n",
    "# Use your existing trainset / encoder / decoder variables from the notebook\n",
    "gym_ppo_env = GymImageTransmissionEnv(trainset, encoder, decoder, DEVICE,\n",
    "                                      preview_dim=16, lambda_penalty=LAMBDA, max_steps=MAX_STEPS_PER_EP)\n",
    "mon = Monitor(gym_ppo_env, filename='outputs/ppo_monitor/monitor.csv', allow_early_resets=True)\n",
    "vec_env = DummyVecEnv([lambda: mon])\n",
    "print('Created PPO vec env and Monitor at outputs/ppo_monitor/monitor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppo-train"
   },
   "outputs": [],
   "source": [
    "# PPO training cell (small defaults). Change hyperparameters here or at the top of the notebook.\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import numpy as _np\n",
    "\n",
    "# ----------------- PPO hyperparameters (change for larger experiments) -----------------\n",
    "PPO_TOTAL_TIMESTEPS = 30000   # nodest default; for serious runs try 1e5 - 1e6\n",
    "PPO_LR = 3e-4\n",
    "PPO_N_STEPS = 2048\n",
    "PPO_BATCH_SIZE = 64\n",
    "PPO_N_EPOCHS = 10\n",
    "PPO_GAMMA = 0.99\n",
    "PPO_POLICY_KWARGS = dict(net_arch=[dict(pi=[256,256], vf=[256,256])])\n",
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "class ScalarLoggingCallback(BaseCallback):\n",
    "    \"\"\"Collect a few training scalars from SB3 logger at the end of each rollout (best-effort).\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(verbose=0)\n",
    "        self.train_logs = []\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        # name_to_value may contain various scalars; collect ones that look like \"train\" or \"loss\"\n",
    "        if hasattr(self.model, 'logger') and hasattr(self.model.logger, 'name_to_value'):\n",
    "            valdict = {}\n",
    "            for k, v in self.model.logger.name_to_value.items():\n",
    "                if k.startswith('train') or k.startswith('loss') or 'policy' in k or 'value' in k:\n",
    "                    try:\n",
    "                        valdict[k] = float(v)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            if valdict:\n",
    "                self.train_logs.append(valdict)\n",
    "\n",
    "# Instantiate PPO model\n",
    "ppo_model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    learning_rate=PPO_LR,\n",
    "    n_steps=PPO_N_STEPS,\n",
    "    batch_size=PPO_BATCH_SIZE,\n",
    "    n_epochs=PPO_N_EPOCHS,\n",
    "    gamma=PPO_GAMMA,\n",
    "    policy_kwargs=PPO_POLICY_KWARGS,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"outputs/ppo_tb\"\n",
    ")\n",
    "scalar_cb = ScalarLoggingCallback()\n",
    "print('Starting PPO training — this may take time depending on PPO_TOTAL_TIMESTEPS and your hardware')\n",
    "ppo_model.learn(total_timesteps=PPO_TOTAL_TIMESTEPS, callback=scalar_cb)\n",
    "ppo_model.save('outputs/ppo_policy')\n",
    "ppo_train_scalars = scalar_cb.train_logs\n",
    "print('Saved PPO model to outputs/ppo_policy.zip and collected', len(ppo_train_scalars), 'rollout scalar entries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppo-load-monitor"
   },
   "outputs": [],
   "source": [
    "# Load PPO Monitor CSV (episode rewards & lengths) for plotting\n",
    "import pandas as pd\n",
    "mon_csv = 'outputs/ppo_monitor/monitor.csv'\n",
    "ppo_reward_hist = []\n",
    "ppo_length_hist = []\n",
    "if os.path.exists(mon_csv):\n",
    "    df = pd.read_csv(mon_csv, comment='#', header=None)\n",
    "    if df.shape[1] >= 2:\n",
    "        ppo_reward_hist = df[0].tolist()\n",
    "        ppo_length_hist = df[1].tolist()\n",
    "    print(f'Loaded PPO monitor: {len(ppo_reward_hist)} episodes')\n",
    "else:\n",
    "    print('PPO monitor CSV not found; make sure Monitor recorded episodes during training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppo-eval-recons"
   },
   "outputs": [],
   "source": [
    "# Evaluate PPO greedy policy on same sample images used elsewhere and save reconstructions + MSEs\n",
    "from torch import no_grad\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "encoder.eval(); decoder.eval();\n",
    "# sample_orig should exist if earlier evaluation cells saved the 25 test images; if not, build them\n",
    "try:\n",
    "    sample_orig\n",
    "except NameError:\n",
    "    # lazy fallback: construct the same 25 grayscale resized CIFAR images used in other notebooks\n",
    "    from torchvision.datasets import CIFAR10\n",
    "    import torchvision.transforms as T\n",
    "    transform = T.Compose([T.Resize((64,64)), T.Grayscale(num_output_channels=1), T.ToTensor()])\n",
    "    testset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "    sample_imgs = []\n",
    "    for i, (img, lbl) in enumerate(testset):\n",
    "        sample_imgs.append(img)\n",
    "        if i >= 24: break\n",
    "    sample_orig = torch.stack(sample_imgs).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z_full = encoder(sample_orig)\n",
    "    rec_full = decoder.decode(z_full, None)\n",
    "\n",
    "ppo_recons = []\n",
    "ppo_ms = []\n",
    "for i in range(sample_orig.size(0)):\n",
    "    obs = z_full[i, :N_PREV if False else 16].cpu().numpy().astype(np.float32)  # keep compatibility with previous code using preview_dim=16\n",
    "    # SB3 predict accepts numpy input\n",
    "    action, _ = ppo_model.predict(obs, deterministic=True)\n",
    "    act = int(action[0]) if hasattr(action, '__len__') else int(action)\n",
    "    K = K_CHOICES[act]\n",
    "    z = encoder(sample_orig[i:i+1])\n",
    "    z_l = z.clone()\n",
    "    if K < z.shape[1]:\n",
    "        z_l[:, K:] = 0.0\n",
    "    rec = decoder.decode(z_l, None)\n",
    "    ppo_recons.append(rec.squeeze(0).cpu())\n",
    "    mse = F.mse_loss(rec, sample_orig[i:i+1]).item()\n",
    "    ppo_ms.append(mse)\n",
    "\n",
    "grid_ppo = make_grid(torch.stack(ppo_recons), nrow=5, pad_value=1.0)\n",
    "save_image(grid_ppo, 'outputs/cifar10_test_25_after_PPO_policy.png')\n",
    "print('Saved outputs/cifar10_test_25_after_PPO_policy.png')\n",
    "\n",
    "# Append/Update outputs/recon_mses.csv — add a column for PPO MSEs\n",
    "import csv\n",
    "csv_path = 'outputs/recon_mses.csv'\n",
    "if os.path.exists(csv_path):\n",
    "    with open(csv_path, 'r') as f:\n",
    "        rows = list(csv.reader(f))\n",
    "    header = rows[0]\n",
    "    old_rows = rows[1:]\n",
    "else:\n",
    "    header = ['idx','mse_ae_full','mse_dqn','mse_a2c']\n",
    "    old_rows = []\n",
    "new_header = header.copy()\n",
    "if 'mse_ppo' not in new_header:\n",
    "    new_header.append('mse_ppo')\n",
    "out_rows = [new_header]\n",
    "for i in range(sample_orig.size(0)):\n",
    "    mse_ae_full = F.mse_loss(rec_full[i:i+1], sample_orig[i:i+1]).item()\n",
    "    mse_d = ''\n",
    "    mse_a = ''\n",
    "    # try to reuse existing values if present\n",
    "    if i < len(old_rows):\n",
    "        try:\n",
    "            orow = old_rows[i]\n",
    "            # map columns conservatively if lengths match\n",
    "            if len(orow) >= 4:\n",
    "                mse_d = orow[2]\n",
    "                mse_a = orow[3]\n",
    "        except Exception:\n",
    "            pass\n",
    "    mse_p = ppo_ms[i]\n",
    "    out_rows.append([i, mse_ae_full, mse_d, mse_a, mse_p])\n",
    "with open(csv_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(out_rows)\n",
    "print('Updated outputs/recon_mses.csv with PPO entries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppo-plots"
   },
   "outputs": [],
   "source": [
    "# Update comparison plots (DQN, A2C, PPO). This cell expects dqn_reward_hist, a2c_reward_hist, ppo_reward_hist etc. to exist.\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1,3,1)\n",
    "if 'dqn_reward_hist' in globals():\n",
    "    plt.plot(dqn_reward_hist, label='DQN', alpha=0.8)\n",
    "if 'a2c_reward_hist' in globals():\n",
    "    plt.plot(np.arange(len(a2c_reward_hist)), a2c_reward_hist, label='A2C', alpha=0.8)\n",
    "if 'ppo_reward_hist' in globals():\n",
    "    plt.plot(np.arange(len(ppo_reward_hist)), ppo_reward_hist, label='PPO', alpha=0.8)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode reward')\n",
    "plt.legend()\n",
    "plt.title('Episode rewards vs training iterations')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "if 'dqn_length_hist' in globals():\n",
    "    plt.plot(dqn_length_hist, label='DQN', alpha=0.8)\n",
    "if 'a2c_length_hist' in globals():\n",
    "    plt.plot(np.arange(len(a2c_length_hist)), a2c_length_hist, label='A2C', alpha=0.8)\n",
    "if 'ppo_length_hist' in globals():\n",
    "    plt.plot(np.arange(len(ppo_length_hist)), ppo_length_hist, label='PPO', alpha=0.8)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode length')\n",
    "plt.legend()\n",
    "plt.title('Episode length vs training iterations')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "def smooth(x, w=5):\n",
    "    if len(x) < w: return x\n",
    "    return np.convolve(x, np.ones(w)/w, mode='valid')\n",
    "sd = smooth(dqn_loss_hist, w=5) if 'dqn_loss_hist' in globals() and len(dqn_loss_hist)>0 else []\n",
    "sa = smooth(a2c_loss_hist, w=5) if 'a2c_loss_hist' in globals() and len(a2c_loss_hist)>0 else []\n",
    "sp = []\n",
    "if 'ppo_train_scalars' in globals() and len(ppo_train_scalars)>0:\n",
    "    first_key = None\n",
    "    for d in ppo_train_scalars:\n",
    "        if d:\n",
    "            first_key = next(iter(d.keys()))\n",
    "            break\n",
    "    if first_key is not None:\n",
    "        sp = [d.get(first_key, np.nan) for d in ppo_train_scalars]\n",
    "        sp = smooth(sp, w=5)\n",
    "if len(sd)>0: plt.plot(sd, label='DQN loss (smoothed)')\n",
    "if len(sa)>0: plt.plot(sa, label='A2C loss (smoothed)')\n",
    "if len(sp)>0: plt.plot(sp, label='PPO loss (smoothed)')\n",
    "plt.xlabel('Training updates (smoothed)')\n",
    "plt.ylabel('Loss (approx)')\n",
    "plt.legend()\n",
    "plt.title('Training loss vs iterations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/comparison_plots_with_PPO.png')\n",
    "print('Saved outputs/comparison_plots_with_PPO.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}