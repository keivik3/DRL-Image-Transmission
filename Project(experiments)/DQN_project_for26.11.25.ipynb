{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Yqj2Dst_AIF9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — imports & hyperparams\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# DQN / env hyperparams\n",
    "CODE_DIM = 128          # must match encoder code_dim\n",
    "N_PREV = 5              # length of context used by decoder\n",
    "K_CHOICES = [128, 96, 64, 32]   # actions = choose K (absolute #latent dims)\n",
    "LAMBDA = 0.01           # penalty on K in reward\n",
    "MAX_EPISODES = 300\n",
    "MAX_STEPS_PER_EP = 400\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "LR_DQN = 1e-3\n",
    "REPLAY_CAP = 10000\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 2000\n",
    "TAU = 0.995   # soft update factor for target network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zIJOnpEDAdOF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: encoder.pth not found. Run AE training first.\n",
      "Warning: decoder.pth not found. Run AE training first.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — load encoder/decoder classes and weights\n",
    "from encoder import Encoder   # expects encoder.py present\n",
    "from decoder import Decoder   # expects decoder.py present\n",
    "\n",
    "# instantiate and load saved weights (from AE training)\n",
    "encoder = Encoder(input_ch=1, code_dim=CODE_DIM).to(DEVICE)\n",
    "decoder = Decoder(code_dim=CODE_DIM, n_prev=N_PREV, output_ch=1).to(DEVICE)\n",
    "\n",
    "if os.path.exists(\"encoder.pth\"):\n",
    "    encoder.load_state_dict(torch.load(\"encoder.pth\", map_location=DEVICE))\n",
    "    print(\"Loaded encoder.pth\")\n",
    "else:\n",
    "\n",
    "    print(\"Warning: encoder.pth not found. Run AE training first.\")\n",
    "\n",
    "if os.path.exists(\"decoder.pth\"):\n",
    "    decoder.load_state_dict(torch.load(\"decoder.pth\", map_location=DEVICE))\n",
    "    print(\"Loaded decoder.pth\")\n",
    "else:\n",
    "\n",
    "    print(\"Warning: decoder.pth not found. Run AE training first.\")\n",
    "\n",
    "# freeze AE weights (we only train DQN)\n",
    "encoder.eval(); decoder.eval()\n",
    "for p in encoder.parameters(): p.requires_grad = False\n",
    "for p in decoder.parameters(): p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MU0zaVKyAdoO"
   },
   "outputs": [],
   "source": [
    "# Cell 3 — LengthEnv: uses encoder & decoder, returns observation vector and reward\n",
    "import gymnasium as gym\n",
    "from PIL import Image\n",
    "\n",
    "class LengthEnv(gym.Env):\n",
    "    \"\"\"\n",
    "\n",
    "    Action: Discrete index selecting K from K_CHOICES.\n",
    "    Observation: concatenation of cart state (4 dims) + first preview_dim elements of current z (float).\n",
    "    Reward: PSNR between reconstructed (using truncated z and previous truncated z buffer) and original frame\n",
    "            minus lambda * (K) penalty (we use K / CODE_DIM scaled or absolute depending on preference).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, device, preview_dim=16, lambda_penalty=LAMBDA, max_steps=400):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.encoder = encoder.to(device)\n",
    "        self.decoder = decoder.to(device)\n",
    "        self.preview_dim = preview_dim\n",
    "        self.lambda_penalty = lambda_penalty\n",
    "        self.max_steps = max_steps\n",
    "        self.length_levels = K_CHOICES\n",
    "        self.action_space = gym.spaces.Discrete(len(self.length_levels))\n",
    "\n",
    "        # CartPole observation ranges (approx)\n",
    "        cart_low = np.array([-4.8, -np.finfo(np.float32).max, -0.418, -np.finfo(np.float32).max], dtype=np.float32)\n",
    "        cart_high = -cart_low\n",
    "        obs_low = np.concatenate([cart_low, np.full(self.preview_dim, -10.0)], axis=0)\n",
    "        obs_high = np.concatenate([cart_high, np.full(self.preview_dim, 10.0)], axis=0)\n",
    "        self.observation_space = gym.spaces.Box(low=obs_low, high=obs_high, dtype=np.float32)\n",
    "\n",
    "        # underlying environment for frames and dynamics\n",
    "        self.env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "        self.reset()\n",
    "\n",
    "    def _preprocess(self, frame):\n",
    "        # frame is RGB ndarray, convert to gray 64x64 normalized [0,1]\n",
    "        img = Image.fromarray(np.asarray(frame)).convert(\"L\").resize((64,64), Image.BILINEAR)\n",
    "        arr = np.asarray(img, dtype=np.float32) / 255.0\n",
    "        arr = arr[None, None, :, :]  # (1,1,H,W)\n",
    "        return torch.tensor(arr, device=self.device)\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        res = self.env.reset(seed=seed)\n",
    "        # gymnasium reset may return (obs, info) or obs only; handle both\n",
    "        if isinstance(res, tuple) and len(res) >= 1:\n",
    "            obs = res[0]\n",
    "        else:\n",
    "            obs = res\n",
    "        self.cart_state = np.array(obs, dtype=np.float32)\n",
    "        frame = self.env.render()\n",
    "        self.frame = self._preprocess(frame)\n",
    "        self.step_count = 0\n",
    "        self.buffer = deque(maxlen=N_PREV)\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # encode current frame to latent and expose preview_dim of it\n",
    "        with torch.no_grad():\n",
    "            z = encoder(self.frame) if callable(encoder) else encoder(self.frame)\n",
    "        preview = z[0, :self.preview_dim].cpu().numpy()\n",
    "        return np.concatenate([self.cart_state, preview], axis=0).astype(np.float32)\n",
    "\n",
    "    def step(self, action_idx):\n",
    "        K = self.length_levels[int(action_idx)]\n",
    "        # encode current frame\n",
    "        with torch.no_grad():\n",
    "            z = encoder(self.frame)  # shape (1, code_dim)\n",
    "        z_limited = z.clone()\n",
    "        if K < z.shape[1]:\n",
    "            z_limited[:, K:] = 0.0\n",
    "\n",
    "        # prepare prev_latents for decoder: if buffer less than N_PREV, use zeros\n",
    "        if len(self.buffer) < N_PREV:\n",
    "            prev = None\n",
    "        else:\n",
    "            prev = torch.cat(list(self.buffer), dim=0).unsqueeze(0).view(1, N_PREV, CODE_DIM).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rec = decoder.decode(z_limited, prev_latents=prev)  # (1,1,H,W)\n",
    "        mse = F.mse_loss(rec, self.frame).item()\n",
    "        psnr = 10.0 * math.log10(1.0 / (mse + 1e-8))\n",
    "        # penalty is proportional to K (absolute) scaled — choose whichever is meaningful\n",
    "        penalty = self.lambda_penalty * (K / float(CODE_DIM))\n",
    "        reward = psnr - penalty\n",
    "\n",
    "        # step the underlying env with a simple balancing policy for continuity (or random)\n",
    "        # here we use a simple heuristic controller for the cart to avoid quick terminations\n",
    "        action_env = 1 if self.cart_state[2] > 0 else 0\n",
    "        step_res = self.env.step(action_env)\n",
    "        # gymnasium step returns (obs, reward, terminated, truncated, info); handle variants\n",
    "        if len(step_res) == 5:\n",
    "            obs, _, terminated, truncated, info = step_res\n",
    "        else:\n",
    "\n",
    "            # legacy gym: obs, reward, done, info\n",
    "            obs, _, done, info = step_res\n",
    "            terminated = done; truncated = False\n",
    "\n",
    "        self.cart_state = np.array(obs, dtype=np.float32)\n",
    "        self.frame = self._preprocess(self.env.render())\n",
    "        self.step_count += 1\n",
    "        done = terminated or truncated or (self.step_count >= self.max_steps)\n",
    "        return self._get_obs(), float(reward), bool(done), False, {}\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n"
   ]
  },
  {
