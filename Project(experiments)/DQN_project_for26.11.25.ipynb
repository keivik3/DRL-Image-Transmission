{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN + A2C (CIFAR-10) integrated notebook\n",
    "\n",
    "This notebook integrates the original DQN pipeline and an A2C implementation, adapts both to CIFAR-10 images (converted to 64x64 grayscale), trains a small autoencoder, then trains both RL algorithms on an ImageTransmission environment that simulates selecting latent truncation K for transmission.\n",
    "\n",
    "Notes:\n",
    "- Encoder and Decoder are defined inline (no external files).\n",
    "- Default (nodest) hyperparameters are chosen to keep runtime reasonable. Change the hyperparameters in the top cell where marked for larger experiments.\n",
    "- The notebook saves sample images and comparison plots to the working directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: imports and (changeable) hyperparameters - change values below for larger experiments\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "# ----------------- Change these for larger experiments -----------------\n",
    "# Autoencoder training\n",
    "AE_EPOCHS = 12            # nodest default; for serious run: try 50+\n",
    "AE_BATCH = 128            # batch size for AE training\n",
    "# RL training\n",
    "DQN_EPISODES = 200        # nodest default; for serious run: try 1000+\n",
    "A2C_EPISODES = 400        # nodest default; for serious run: try 1000+\n",
    "STEPS_PER_EP = 50         # how many images per episode (short for demo)\n",
    "# Preview / AE / RL sizes\n",
    "CODE_DIM = 128            # must match encoder/decoder code_dim\n",
    "N_PREV = 5                # number of previous latents used by decoder\n",
    "PREVIEW_DIM = 16          # how many elements of z we expose as observation\n",
    "K_CHOICES = [128, 96, 64, 32]\n",
    "LAMBDA = 0.01             # penalty on K in reward\n",
    "# DQN hyperparams\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "LR_DQN = 1e-3\n",
    "REPLAY_CAP = 10000\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 2000\n",
    "TAU = 0.995\n",
    "# A2C hyperparams\n",
    "LR_A2C = 1e-3\n",
    "N_STEPS = 10              # n-step bootstrap length; increase for performance\n",
    "PRINT_INTERVAL = 20\n",
    "SEED = 42\n",
    "# ---------------------------------------------------------------------\n",
    "torch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Inline Encoder and Decoder (integrated instead of separate files)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_ch=1, code_dim=128):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_ch, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.fc_enc = None\n",
    "        self.code_dim = code_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        feat = self.conv(x)\n",
    "        if self.fc_enc is None:\n",
    "            flat_dim = feat.numel() // B\n",
    "            self.fc_enc = nn.Linear(flat_dim, self.code_dim).to(x.device)\n",
    "            print(f\"âœ… Initialized fc_enc with input size {flat_dim}\")\n",
    "        flat = feat.reshape(B, -1)\n",
    "        z = self.fc_enc(flat)\n",
    "        return z\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, code_dim=128, n_prev=5, output_ch=1):\n",
    "        super().__init__()\n",
    "        self.n_prev = n_prev\n",
    "        self.code_dim = code_dim\n",
    "        total_dim = (n_prev + 1) * code_dim\n",
    "\n",
    "        self.fc_dec = nn.Linear(total_dim, 64 * 8 * 8)\n",
    "\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(16, output_ch, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def decode(self, z, prev_latents=None):\n",
    "        if prev_latents is not None:\n",
    "            B = z.size(0)\n",
    "            prev = prev_latents.view(B, self.n_prev * self.code_dim)\n",
    "            z_cat = torch.cat([z, prev], dim=1)\n",
    "        else:\n",
    "            B = z.size(0)\n",
    "            zeros = torch.zeros(B, self.n_prev * self.code_dim, device=z.device)\n",
    "            z_cat = torch.cat([z, zeros], dim=1)\n",
    "\n",
    "        feat = self.fc_dec(z_cat)\n",
    "        feat = feat.view(-1, 64, 8, 8)\n",
    "        rec = self.deconv(feat)\n",
    "        return rec\n",
    "\n",
    "    def forward(self, z, prev_latents=None):\n",
    "        return self.decode(z, prev_latents)\n",
    "\n",
    "# instantiate AE models\n",
    "encoder = Encoder(input_ch=1, code_dim=CODE_DIM).to(DEVICE)\n",
    "decoder = Decoder(code_dim=CODE_DIM, n_prev=N_PREV, output_ch=1).to(DEVICE)\n",
    "\n",
    "# Print small sanity check\n",
    "x = torch.randn(2,1,64,64, device=DEVICE)\n",
    "with torch.no_grad():\n",
    "    z = encoder(x)\n",
    "    rec = decoder.decode(z, None)\n",
    "print('AE forward shapes:', z.shape, rec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load CIFAR-10, convert to grayscale 64x64, show sample grid\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((64,64)),\n",
    "    T.Grayscale(num_output_channels=1),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "trainset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=AE_BATCH, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=AE_BATCH, shuffle=False, num_workers=2)\n",
    "\n",
    "# save first 25 test images grid (originals)\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "sample_imgs = []\n",
    "for i, (img, lbl) in enumerate(testset):\n",
    "    sample_imgs.append(img)\n",
    "    if i >= 24:\n",
    "        break\n",
    "grid = make_grid(torch.stack(sample_imgs), nrow=5, pad_value=1.0)\n",
    "save_image(grid, 'outputs/cifar10_test_25_originals.png')\n",
    "print('Saved outputs/cifar10_test_25_originals.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Train small autoencoder (MSE) - default modest epochs, change AE_EPOCHS above for longer training\n",
    "ae_optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-3)\n",
    "ae_loss_hist = []\n",
    "\n",
    "for epoch in range(1, AE_EPOCHS+1):\n",
    "    running = 0.0\n",
    "    for imgs, _ in train_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        ae_optimizer.zero_grad()\n",
    "        z = encoder(imgs)\n",
    "        rec = decoder.decode(z, None)\n",
    "        loss = F.mse_loss(rec, imgs)\n",
    "        loss.backward()\n",
    "        ae_optimizer.step()\n",
    "        running += loss.item() * imgs.size(0)\n",
    "    epoch_loss = running / len(trainset)\n",
    "    ae_loss_hist.append(epoch_loss)\n",
    "    print(f'AE Epoch {epoch}/{AE_EPOCHS} | loss = {epoch_loss:.6f}')\n",
    "\n",
    "# save a reconstruction grid after AE training (full code)\n",
    "encoder.eval(); decoder.eval()\n",
    "with torch.no_grad():\n",
    "    sample_batch = torch.stack(sample_imgs).to(DEVICE)\n",
    "    z = encoder(sample_batch)\n",
    "    rec_full = decoder.decode(z, None)\n",
    "    grid_rec = make_grid(rec_full.cpu(), nrow=5, pad_value=1.0)\n",
    "    save_image(grid_rec, 'outputs/cifar10_test_25_after_AE.png')\n",
    "print('Saved outputs/cifar10_test_25_after_AE.png')\n",
    "# also save AE models to disk (optional)\n",
    "torch.save(encoder.state_dict(), 'outputs/encoder.pth')\n",
    "torch.save(decoder.state_dict(), 'outputs/decoder.pth')\n",
    "print('Saved AE weights to outputs/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: ImageTransmissionEnv (uses trainset iterator)\n",
    "class ImageTransmissionEnv:\n",
    "    def __init__(self, dataset, encoder, decoder, device, preview_dim=PREVIEW_DIM, lambda_penalty=LAMBDA, max_steps=STEPS_PER_EP):\n",
    "        self.dataset = dataset\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.preview_dim = preview_dim\n",
    "        self.lambda_penalty = lambda_penalty\n",
    "        self.max_steps = max_steps\n",
    "        self.length_levels = K_CHOICES\n",
    "        self.idx = 0\n",
    "        self.step_count = 0\n",
    "        self.buffer = deque(maxlen=N_PREV)\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_count = 0\n",
    "        self.buffer.clear()\n",
    "        # start from current idx image\n",
    "        img, _ = self.dataset[self.idx]\n",
    "        self.current_img = img.to(self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            z = self.encoder(self.current_img)\n",
    "        self.buffer.append(z.detach().cpu())\n",
    "        obs = z[0, :self.preview_dim].cpu().numpy().astype(np.float32)\n",
    "        return obs\n",
    "\n",
    "    def step(self, action_idx):\n",
    "        K = self.length_levels[int(action_idx)]\n",
    "        with torch.no_grad():\n",
    "            z = self.encoder(self.current_img)\n",
    "        z_limited = z.clone()\n",
    "        if K < z.shape[1]:\n",
    "            z_limited[:, K:] = 0.0\n",
    "        prev = None if len(self.buffer) < N_PREV else torch.cat(list(self.buffer), dim=0).unsqueeze(0).view(1, N_PREV, CODE_DIM).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            rec = self.decoder.decode(z_limited.to(self.device), prev_latents=prev)\n",
    "        mse = F.mse_loss(rec, self.current_img).item()\n",
    "        psnr = 10.0 * math.log10(1.0 / (mse + 1e-8))\n",
    "        penalty = self.lambda_penalty * (K / float(CODE_DIM))\n",
    "        reward = psnr - penalty\n",
    "        # advance to next image in dataset (circular)\n",
    "        self.idx = (self.idx + 1) % len(self.dataset)\n",
    "        img, _ = self.dataset[self.idx]\n",
    "        self.current_img = img.to(self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            z_next = self.encoder(self.current_img)\n",
    "        self.buffer.append(z_next.detach().cpu())\n",
    "        self.step_count += 1\n",
    "        done = (self.step_count >= self.max_steps)\n",
    "        obs = z_next[0, :self.preview_dim].cpu().numpy().astype(np.float32)\n",
    "        return obs, float(reward), bool(done), {}\n",
    "\n",
    "    def seed(self, s):\n",
    "        random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
    "\n",
    "env = ImageTransmissionEnv(trainset, encoder, decoder, DEVICE)\n",
    "obs_dim = PREVIEW_DIM\n",
    "n_actions = len(K_CHOICES)\n",
    "print('Env obs_dim, n_actions =', obs_dim, n_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: DQN (adapted to ImageTransmissionEnv)\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_obs, n_actions):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_obs, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "policy_net = DQN(obs_dim, n_actions).to(DEVICE)\n",
    "target_net = DQN(obs_dim, n_actions).to(DEVICE)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer_dqn = optim.Adam(policy_net.parameters(), lr=LR_DQN)\n",
    "memory = ReplayMemory(REPLAY_CAP)\n",
    "steps_done = 0\n",
    "dqn_reward_hist = []\n",
    "dqn_length_hist = []\n",
    "dqn_loss_hist = []\n",
    "\n",
    "def select_action_dqn(state):\n",
    "    global steps_done\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if random.random() > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            s = torch.tensor(state, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "            return int(policy_net(s).argmax(dim=1).item())\n",
    "    else:\n",
    "        return random.randrange(n_actions)\n",
    "\n",
    "def optimize_model_dqn():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return None\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    state_batch = torch.tensor(np.stack(batch.state), dtype=torch.float32, device=DEVICE)\n",
    "    action_batch = torch.tensor(batch.action, dtype=torch.int64, device=DEVICE).unsqueeze(1)\n",
    "    reward_batch = torch.tensor(batch.reward, dtype=torch.float32, device=DEVICE).unsqueeze(1)\n",
    "    non_final_mask = torch.tensor([s is not None for s in batch.next_state], device=DEVICE, dtype=torch.bool)\n",
    "    non_final_next = torch.tensor([s for s in batch.next_state if s is not None], dtype=torch.float32, device=DEVICE) if any(non_final_mask) else None\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, 1, device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        if non_final_next is not None:\n",
    "            next_state_values[non_final_mask] = target_net(non_final_next).max(1)[0].unsqueeze(1)\n",
    "    expected_values = reward_batch + GAMMA * next_state_values\n",
    "    loss = nn.SmoothL1Loss()(state_action_values, expected_values.detach())\n",
    "    optimizer_dqn.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 5.0)\n",
    "    optimizer_dqn.step()\n",
    "    return loss.item()\n",
    "\n",
    "# DQN training loop (modest defaults)\n",
    "for ep in range(1, DQN_EPISODES+1):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0\n",
    "    for t in range(STEPS_PER_EP):\n",
    "        action = select_action_dqn(obs)\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        memory.push(obs, action, None if done else next_obs, reward)\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        l = optimize_model_dqn()\n",
    "        if l is not None:\n",
    "            dqn_loss_hist.append(l)\n",
    "        # soft update\n",
    "        for tp, pp in zip(target_net.parameters(), policy_net.parameters()):\n",
    "            tp.data.copy_(TAU * pp.data + (1.0 - TAU) * tp.data)\n",
    "        if done:\n",
    "            break\n",
    "    dqn_reward_hist.append(total_reward)\n",
    "    dqn_length_hist.append(t+1)\n",
    "    if ep % 20 == 0:\n",
    "        print(f'DQN Episode {ep}/{DQN_EPISODES} | Reward: {total_reward:.2f} | avg(last20) = {np.mean(dqn_reward_hist[-20:]):.2f}')\n",
    "\n",
    "torch.save(policy_net.state_dict(), 'outputs/dqn_policy.pth')\n",
    "print('Saved outputs/dqn_policy.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: A2C implementation (on same ImageTransmissionEnv)\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, n_actions):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.policy_head = nn.Linear(128, n_actions)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        logits = self.policy_head(x)\n",
    "        value = self.value_head(x).squeeze(-1)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        dist = Categorical(probs=probs)\n",
    "        return dist, value\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        logits = self.policy_head(x)\n",
    "        value = self.value_head(x).squeeze(-1)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        dist = Categorical(probs=probs)\n",
    "        return dist, value\n",
    "\n",
    "ac = ActorCritic(obs_dim, n_actions).to(DEVICE)\n",
    "optimizer_ac = optim.Adam(ac.parameters(), lr=LR_A2C)\n",
    "\n",
    "a2c_reward_hist = []\n",
    "a2c_length_hist = []\n",
    "a2c_loss_hist = []\n",
    "\n",
    "# A2C training loop\n",
    "for ep in range(1, A2C_EPISODES+1):\n",
    "    obs = env.reset()\n",
    "    obs = torch.tensor(obs, dtype=torch.float32, device=DEVICE)\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    step_in_ep = 0\n",
    "    obs_buf = []\n",
    "    logprob_buf = []\n",
    "    value_buf = []\n",
    "    reward_buf = []\n",
    "    done_buf = []\n",
    "    while not done and step_in_ep < STEPS_PER_EP:\n",
    "        step_in_ep += 1\n",
    "        dist, value = ac(obs.unsqueeze(0))\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        next_obs, reward, done, info = env.step(int(action.item()))\n",
    "        obs_buf.append(obs)\n",
    "        logprob_buf.append(log_prob.squeeze(0))\n",
    "        value_buf.append(value.squeeze(0))\n",
    "        reward_buf.append(reward)\n",
    "        done_buf.append(float(done))\n",
    "        total_reward += reward\n",
    "        obs = torch.tensor(next_obs, dtype=torch.float32, device=DEVICE)\n",
    "        if len(reward_buf) == N_STEPS or done:\n",
    "            with torch.no_grad():\n",
    "                if done:\n",
    "                    next_value = torch.tensor(0.0, device=DEVICE)\n",
    "                else:\n",
    "                    _, nv = ac(obs.unsqueeze(0))\n",
    "                    next_value = nv.squeeze(0)\n",
    "            values = torch.stack(value_buf)\n",
    "            log_probs = torch.stack(logprob_buf)\n",
    "            rewards = torch.tensor(reward_buf, dtype=torch.float32, device=DEVICE)\n",
    "            dones = torch.tensor(done_buf, dtype=torch.float32, device=DEVICE)\n",
    "            T = rewards.shape[0]\n",
    "            targets = torch.zeros(T, device=values.device)\n",
    "            G = next_value\n",
    "            for t in reversed(range(T)):\n",
    "                G = rewards[t] + GAMMA * G * (1.0 - dones[t])\n",
    "                targets[t] = G\n",
    "            advantages = targets - values.detach()\n",
    "            actor_loss = -(log_probs * advantages).mean()\n",
    "            critic_loss = F.mse_loss(values, targets)\n",
    "            loss = actor_loss + 0.5 * critic_loss\n",
    "            optimizer_ac.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(ac.parameters(), 5.0)\n",
    "            optimizer_ac.step()\n",
    "            a2c_loss_hist.append(loss.item())\n",
    "            obs_buf.clear(); logprob_buf.clear(); value_buf.clear(); reward_buf.clear(); done_buf.clear()\n",
    "    a2c_reward_hist.append(total_reward)\n",
    "    a2c_length_hist.append(step_in_ep)\n",
    "    if ep % PRINT_INTERVAL == 0:\n",
    "        print(f'A2C Episode {ep}/{A2C_EPISODES} | avg(last{PRINT_INTERVAL}) = {np.mean(a2c_reward_hist[-PRINT_INTERVAL:]):.2f} | ep_len = {step_in_ep}')\n",
    "\n",
    "torch.save(ac.state_dict(), 'outputs/a2c_ac.pth')\n",
    "print('Saved outputs/a2c_ac.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Evaluate policies on the first 25 test images and save reconstructions; compute per-image MSEs\n",
    "import torchvision.utils as vutils\n",
    "encoder.eval(); decoder.eval(); policy_net.eval(); ac.eval()\n",
    "sample_orig = torch.stack(sample_imgs).to(DEVICE)\n",
    "# full AE reconstructions (already saved earlier)\n",
    "with torch.no_grad():\n",
    "    z_full = encoder(sample_orig)\n",
    "    rec_full = decoder.decode(z_full, None)\n",
    "# DQN reconstructions: use greedy policy (argmax) per sample\n",
    "dqn_recons = []\n",
    "dqn_ms = []\n",
    "for i in range(sample_orig.size(0)):\n",
    "    obs = z_full[i, :PREVIEW_DIM].cpu().numpy().astype(np.float32)\n",
    "    s = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        act = int(policy_net(s).argmax(dim=1).item())\n",
    "        K = K_CHOICES[act]\n",
    "        z = encoder(sample_orig[i:i+1])\n",
    "        z_l = z.clone()\n",
    "        if K < z.shape[1]:\n",
    "            z_l[:, K:] = 0.0\n",
    "        rec = decoder.decode(z_l, None)\n",
    "        dqn_recons.append(rec.squeeze(0).cpu())\n",
    "        mse = F.mse_loss(rec, sample_orig[i:i+1]).item()\n",
    "        dqn_ms.append(mse)\n",
    "# A2C reconstructions: use greedy (argmax) of actor probs\n",
    "a2c_recons = []\n",
    "a2c_ms = []\n",
    "for i in range(sample_orig.size(0)):\n",
    "    obs = z_full[i, :PREVIEW_DIM].cpu().numpy().astype(np.float32)\n",
    "    s = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        dist, _ = ac(s)\n",
    "        act = int(dist.probs.argmax(dim=1).item())\n",
    "        K = K_CHOICES[act]\n",
    "        z = encoder(sample_orig[i:i+1])\n",
    "        z_l = z.clone()\n",
    "        if K < z.shape[1]:\n",
    "            z_l[:, K:] = 0.0\n",
    "        rec = decoder.decode(z_l, None)\n",
    "        a2c_recons.append(rec.squeeze(0).cpu())\n",
    "        mse = F.mse_loss(rec, sample_orig[i:i+1]).item()\n",
    "        a2c_ms.append(mse)\n",
    "# Save grids\n",
    "grid_dqn = make_grid(torch.stack(dqn_recons), nrow=5, pad_value=1.0)\n",
    "grid_a2c = make_grid(torch.stack(a2c_recons), nrow=5, pad_value=1.0)\n",
    "save_image(grid_dqn, 'outputs/cifar10_test_25_after_DQN_policy.png')\n",
    "save_image(grid_a2c, 'outputs/cifar10_test_25_after_A2C_policy.png')\n",
    "print('Saved DQN/A2C reconstruction grids')\n",
    "# compute and print MSEs\n",
    "print('Mean MSE AE (full):', F.mse_loss(rec_full, sample_orig).item())\n",
    "print('Mean MSE DQN:', np.mean(dqn_ms), 'Mean MSE A2C:', np.mean(a2c_ms))\n",
    "# save per-image MSEs to CSV\n",
    "import csv\n",
    "with open('outputs/recon_mses.csv', 'w', newline='') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['idx','mse_ae_full','mse_dqn','mse_a2c'])\n",
    "    for i in range(sample_orig.size(0)):\n",
    "        w.writerow([i, F.mse_loss(rec_full[i:i+1], sample_orig[i:i+1]).item(), dqn_ms[i], a2c_ms[i]])\n",
    "print('Saved outputs/recon_mses.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Comparison plots (rewards, episode lengths, losses)\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "\n",
    "plt.plot(dqn_reward_hist, label='DQN')\n",
    "plt.plot(np.linspace(0, len(a2c_reward_hist)-1, len(dqn_reward_hist)), a2c_reward_hist[:len(dqn_reward_hist)], label='A2C')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode reward')\n",
    "plt.legend()\n",
    "plt.title('Episode rewards vs training iterations')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(dqn_length_hist, label='DQN')\n",
    "plt.plot(np.linspace(0, len(a2c_length_hist)-1, len(dqn_length_hist)), a2c_length_hist[:len(dqn_length_hist)], label='A2C')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode length')\n",
    "plt.legend()\n",
    "plt.title('Episode length vs training iterations')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "# For losses align lengths by truncation/padding for display - use moving average smoothing\n",
    "def smooth(x, w=5):\n",
    "    if len(x) < w: return x\n",
    "    return np.convolve(x, np.ones(w)/w, mode='valid')\n",
    "sd = smooth(dqn_loss_hist, w=5) if len(dqn_loss_hist) > 0 else []\n",
    "sa = smooth(a2c_loss_hist, w=5) if len(a2c_loss_hist) > 0 else []\n",
    "if len(sd)>0: plt.plot(sd, label='DQN loss (smoothed)')\n",
    "if len(sa)>0: plt.plot(sa, label='A2C loss (smoothed)')\n",
    "plt.xlabel('Training updates (smoothed)')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training loss vs iterations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/comparison_plots.png')\n",
    "print('Saved outputs/comparison_plots.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps and tuning\n",
    "\n",
    "\n",
    "\"To run a full experiment increase AE_EPOCHS (e.g., 50-200), DQN_EPISODES (e.g., 1000+), and A2C_EPISODES (e.g., 1000+).\n",
    "Also increase STEPS_PER_EP if you want longer episodes (e.g., 200-400).\n",
    "Use a GPU for practical runtimes.\n",
    "\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
