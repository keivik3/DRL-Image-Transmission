{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NygtQ_H3nlWY",
        "outputId": "b696de51-3420-4781-ac7a-9e32ec212be7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gymnasium\n",
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "obs, info = env.reset()\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    frame = env.render()\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated\n"
      ],
      "metadata": {
        "id": "Uf7mWEVfq6EA"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xCetfVXvodm",
        "outputId": "acf5ac0d-3170-4620-f7f5-fd3e0fec00c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[classic_control] in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (0.0.4)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (2.6.1)\n"
          ]
        }
      ],
      "source": [
        "pip install gymnasium[classic_control]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install imageio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLRw9ZzzZTbD",
        "outputId": "d7cbc2c7-9a01-42e4-ea90-93a32c4e5bc7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imageio in /usr/local/lib/python3.12/dist-packages (2.37.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from imageio) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio) (11.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile encoder.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_ch=1, code_dim=128):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_ch, 16, 3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.fc_enc = None\n",
        "        self.code_dim = code_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        feat = self.conv(x)\n",
        "        if self.fc_enc is None:\n",
        "            flat_dim = feat.numel() // B\n",
        "            self.fc_enc = nn.Linear(flat_dim, self.code_dim).to(x.device)\n",
        "            print(f\"‚úÖ Initialized fc_enc with input size {flat_dim}\")\n",
        "        flat = feat.reshape(B, -1)\n",
        "        z = self.fc_enc(flat)\n",
        "        return z\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tf0S-0TRcWBf",
        "outputId": "bdeb5741-a5f2-4c56-b75a-809521aed93c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting encoder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile decoder.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, code_dim=128, n_prev=5, output_ch=1):\n",
        "        super().__init__()\n",
        "        self.n_prev = n_prev\n",
        "        self.code_dim = code_dim\n",
        "        total_dim = (n_prev + 1) * code_dim\n",
        "        self.fc_dec = nn.Linear(total_dim, 64 * 8 * 8)\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(16, output_ch, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, z, prev_latents=None):\n",
        "        if prev_latents is not None and len(prev_latents) > 0:\n",
        "            prev_cat = torch.cat(prev_latents, dim=1)\n",
        "            z = torch.cat([z, prev_cat], dim=1)\n",
        "        else:\n",
        "            B = z.size(0)\n",
        "            z = torch.cat([z, torch.zeros(B, self.n_prev * self.code_dim, device=z.device)], dim=1)\n",
        "\n",
        "        feat = self.fc_dec(z).view(-1, 64, 8, 8)\n",
        "        rec = self.deconv(feat)\n",
        "        return rec\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t27J355tcQkE",
        "outputId": "5aaa5ae1-435a-4737-d530-1e95a1742f13"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting decoder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Checking encoder.py and decoder.py\n",
        "# ===============================\n",
        "import torch\n",
        "from encoder import Encoder\n",
        "from decoder import Decoder\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "encoder = Encoder(code_dim=128).to(DEVICE)\n",
        "decoder = Decoder(code_dim=128, n_prev=5).to(DEVICE)\n",
        "\n",
        "x = torch.rand(1, 1, 64, 64).to(DEVICE)\n",
        "z = encoder(x)\n",
        "rec = decoder(z)\n",
        "\n",
        "print(\"Input:\", x.shape)\n",
        "print(\"Latent:\", z.shape)\n",
        "print(\"Reconstructed:\", rec.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-cGbb2acTvv",
        "outputId": "71e6e1bc-b811-48c0-e60e-5c6efb80a183"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Initialized fc_enc with input size 4096\n",
            "Input: torch.Size([1, 1, 64, 64])\n",
            "Latent: torch.Size([1, 128])\n",
            "Reconstructed: torch.Size([1, 1, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QQauhA8vrAn",
        "outputId": "3f61cb45-6f2e-4442-97dd-5c208b4182ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# >>> Cell 1: imports + hyperparams\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# hyperparams\n",
        "CODE_DIM = 128\n",
        "N_PREV = 5\n",
        "IMG_H = IMG_W = 64\n",
        "BATCH_SEQ = 1   # we process sequentially per frame; for AE training we'll use batch 1 per time-step\n",
        "AE_EPOCHS = 12\n",
        "TEACHER_FORCING_EPOCHS = 5    # use GT prev latents for these many epochs (teacher forcing)\n",
        "SCHEDULED_SAMPLING_DECAY = 0.95  # multiply p_tf each epoch after TF phase\n",
        "K_CHOICES = [128, 96, 64, 32]   # absolute K's\n",
        "LAMBDA = 0.01  # penalty coefficient for K in reward (tune)\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "# file names (adjust to your paths)\n",
        "FRAMES_NPY = \"cartpole_frames.npy\"  # if using cartpole frames saved earlier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Q62nVJ4pwtOi"
      },
      "outputs": [],
      "source": [
        "# >>> Cell 2: helper utilities (truncation, psnr)\n",
        "import math\n",
        "\n",
        "def apply_truncation(z, K):\n",
        "    \"\"\"\n",
        "    z: (B, code_dim)\n",
        "    K: int (<= code_dim)\n",
        "    returns z_limited with zeros after index K\n",
        "    \"\"\"\n",
        "    if K >= z.shape[1]:\n",
        "        return z\n",
        "    z_l = z.clone()\n",
        "    z_l[:, K:] = 0.0\n",
        "    return z_l\n",
        "\n",
        "def psnr_from_mse(mse, max_val=1.0):\n",
        "    # mse is scalar or numpy\n",
        "    return 10.0 * math.log10(max_val * max_val / (mse + 1e-10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ww23GyBzw3Iz",
        "outputId": "5fafc011-3ad4-4b30-9021-6745d45e2fcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): ReLU(inplace=True)\n",
            "  )\n",
            ")\n",
            "Decoder(\n",
            "  (fc_dec): Linear(in_features=768, out_features=4096, bias=True)\n",
            "  (deconv): Sequential(\n",
            "    (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (5): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# >>> Cell 3: import Encoder and Decoder\n",
        "from encoder import Encoder\n",
        "from decoder import Decoder\n",
        "\n",
        "encoder = Encoder(input_ch=1, code_dim=CODE_DIM).to(DEVICE)\n",
        "decoder = Decoder(output_ch=1, code_dim=CODE_DIM, n_prev=N_PREV).to(DEVICE)\n",
        "print(encoder)\n",
        "print(decoder)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debug + robust CartPole frames generator (works with various gym versions)\n",
        "import os\n",
        "import gym\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import imageio\n",
        "\n",
        "FRAMES_NPY = \"cartpole_frames.npy\"\n",
        "N_FRAMES = 2000\n",
        "SAVE_GIF = True  # —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ø—Ä–µ–≤—å—é .gif —Å –ø–µ—Ä–≤—ã–º–∏ 10 –∫–∞–¥—Ä–∞–º–∏\n",
        "\n",
        "if not os.path.exists(FRAMES_NPY):\n",
        "    print(\"üé• Creating CartPole video dataset (robust mode)...\")\n",
        "    # —Å–æ–∑–¥–∞—ë–º env ‚Äî —Å—Ç–∞—Ä—ã–π gym –æ–±—ã—á–Ω–æ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç render_mode=None –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç array –∏–∑ env.render()\n",
        "    # –µ—Å–ª–∏ —Å–≤—è–∑–∫–∞ gym/gymnasium —Å–º–µ—à–∞–ª–∞—Å—å, —ç—Ç–æ—Ç –∫–æ–¥ –≤—Å—ë —Ä–∞–≤–Ω–æ –ø–æ—Å—Ç–∞—Ä–∞–µ—Ç—Å—è –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å frame\n",
        "    try:\n",
        "        env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "    except TypeError:\n",
        "        # —Å—Ç–∞—Ä—ã–µ gym –º–æ–≥—É—Ç –Ω–µ –ø—Ä–∏–Ω—è—Ç—å render_mode –≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–µ\n",
        "        env = gym.make(\"CartPole-v1\")\n",
        "    frames = []\n",
        "\n",
        "    # old gym: env.reset() -> obs OR (obs, info) for newer landscape. –ø–æ–ø—Ä–æ–±—É–µ–º –æ–±–æ–∏—Ö:\n",
        "    reset_res = env.reset()\n",
        "    # –£–¥–æ–±–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–µ obs (–≤ –ª—é–±–æ–º —Ñ–æ—Ä–º–∞—Ç–µ)\n",
        "    if isinstance(reset_res, tuple):\n",
        "        obs = reset_res[0]\n",
        "    else:\n",
        "        obs = reset_res\n",
        "\n",
        "    for i in range(N_FRAMES):\n",
        "        frame = env.render()  # –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å np.ndarray –∏–ª–∏ list –∏–ª–∏ PIL image-like\n",
        "\n",
        "        # ---- Debug prints for first few iterations ----\n",
        "        if i < 3:\n",
        "            print(f\"DEBUG: step {i} | type(frame) = {type(frame)}\")\n",
        "            # –µ—Å–ª–∏ —ç—Ç–æ numpy array ‚Äî –≤—ã–≤–µ–¥–µ–º —Ñ–æ—Ä–º—É; –µ—Å–ª–∏ list ‚Äî —Ä–∞–∑–º–µ—Ä, –ø—Ä–∏–º–µ—Ä\n",
        "            try:\n",
        "                arr_try = np.array(frame)\n",
        "                print(\"DEBUG: -> np.array(frame).shape =\", arr_try.shape, \"dtype=\", arr_try.dtype)\n",
        "            except Exception as e:\n",
        "                print(\"DEBUG: cannot convert frame to np.array directly:\", e)\n",
        "            # show a tiny repr snippet:\n",
        "            rep = repr(frame)\n",
        "            print(\"DEBUG: repr(frame)[:200] =\", rep[:200])\n",
        "\n",
        "        # ---- Convert robustly to numpy array ----\n",
        "        # –ï—Å–ª–∏ frame ‚Äî PIL Image, —Å–¥–µ–ª–∞—Ç—å np.array(frame)\n",
        "        if hasattr(frame, \"convert\"):  # PIL Image-like\n",
        "            frame_arr = np.array(frame)\n",
        "        else:\n",
        "            # try to coerce to np.array\n",
        "            frame_arr = np.asarray(frame)\n",
        "\n",
        "        # now ensure frame_arr is numeric np array\n",
        "        if frame_arr.ndim == 2:\n",
        "            # already grayscale HxW\n",
        "            gray = frame_arr.astype(np.uint8)\n",
        "            gray = np.expand_dims(gray, -1)  # H W 1\n",
        "        elif frame_arr.ndim == 3:\n",
        "            # Could be HxWx3 RGB, or HxWx4 RGBA; take first 3\n",
        "            if frame_arr.shape[2] >= 3:\n",
        "                rgb = frame_arr[..., :3].astype(np.float32)\n",
        "                # convert to grayscale\n",
        "                gray = (0.2989 * rgb[...,0] + 0.5870 * rgb[...,1] + 0.1140 * rgb[...,2]).astype(np.uint8)\n",
        "                gray = np.expand_dims(gray, -1)\n",
        "            else:\n",
        "                # unexpected channels, collapse to single channel\n",
        "                gray = frame_arr[..., 0:1].astype(np.uint8)\n",
        "        else:\n",
        "            # fallback: try flatten/pad or raise clearer error\n",
        "            raise RuntimeError(f\"Cannot interpret frame array with ndim={frame_arr.ndim}, repr first chars: {repr(frame)[:200]}\")\n",
        "\n",
        "        frames.append(gray)\n",
        "\n",
        "        # step environment ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å old API: env.step(action) -> (obs, reward, done, info)\n",
        "        action = env.action_space.sample()\n",
        "        step_res = env.step(action)\n",
        "        # handle both old and new API\n",
        "        if isinstance(step_res, tuple):\n",
        "            if len(step_res) == 4:\n",
        "                obs, reward, done, info = step_res\n",
        "            elif len(step_res) == 5:  # new gymnasium: (obs, reward, terminated, truncated, info)\n",
        "                obs, reward, terminated, truncated, info = step_res\n",
        "                done = terminated or truncated\n",
        "            else:\n",
        "                # fallback\n",
        "                obs = step_res[0]\n",
        "                done = False\n",
        "        else:\n",
        "            # unexpected type\n",
        "            done = False\n",
        "\n",
        "        if done:\n",
        "            # reset, keep going\n",
        "            reset_res = env.reset()\n",
        "            obs = reset_res[0] if isinstance(reset_res, tuple) else reset_res\n",
        "\n",
        "    env.close()\n",
        "    if isinstance(frame, list) and len(frame) == 1:\n",
        "      frame_arr = np.asarray(frame[0])\n",
        "    else:\n",
        "      frame_arr = np.asarray(frame)\n",
        "\n",
        "    np.save(FRAMES_NPY, frames)\n",
        "    print(f\"‚úÖ Saved {FRAMES_NPY}, shape {frames.shape}\")\n",
        "else:\n",
        "    print(\"Found existing\", FRAMES_NPY)\n",
        "    frames = np.load(FRAMES_NPY)\n",
        "    print(\"Loaded shape:\", frames.shape)\n",
        "\n",
        "# Optional: save first 10 frames as gif for quick visual check\n",
        "if SAVE_GIF:\n",
        "    preview = (frames[:10, ..., 0]).astype(np.uint8)  # (10, H, W)\n",
        "    pil_frames = [Image.fromarray(f) for f in preview]\n",
        "    pil_frames[0].save(\"preview_cartpole.gif\", save_all=True, append_images=pil_frames[1:], duration=100, loop=0)\n",
        "    print(\"Saved preview_cartpole.gif (first 10 frames).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "SDRbVNHJnV33",
        "outputId": "b6fac434-d195-4556-ce26-e0779b87cc4e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üé• Creating CartPole video dataset (robust mode)...\n",
            "DEBUG: step 0 | type(frame) = <class 'list'>\n",
            "DEBUG: -> np.array(frame).shape = (1, 400, 600, 3) dtype= uint8\n",
            "DEBUG: repr(frame)[:200] = [array([[[255, 255, 255],\n",
            "        [255, 255, 255],\n",
            "        [255, 255, 255],\n",
            "        ...,\n",
            "        [255, 255, 255],\n",
            "        [255, 255, 255],\n",
            "        [255, 255, 255]],\n",
            "\n",
            "       [[255, 255, 255],\n",
            "        [\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Cannot interpret frame array with ndim=4, repr first chars: [array([[[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       [[255, 255, 255],\n        [",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1078280355.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m# fallback: try flatten/pad or raise clearer error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cannot interpret frame array with ndim={frame_arr.ndim}, repr first chars: {repr(frame)[:200]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Cannot interpret frame array with ndim=4, repr first chars: [array([[[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       [[255, 255, 255],\n        ["
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZWH3C0pw-Po"
      },
      "outputs": [],
      "source": [
        "# >>> Cell 4: load dataset (npy) or Kaggle dataset snippet\n",
        "\n",
        "# If you have a local .npy of frames: load it\n",
        "if os.path.exists(FRAMES_NPY):\n",
        "    frames = np.load(FRAMES_NPY)   # shape expected (T, H, W, C) where C=1 or 3\n",
        "    print(\"Loaded frames shape:\", frames.shape)\n",
        "else:\n",
        "    # Example: how to download Kaggle dataset via 'kaggle' library ‚Äî uncomment and adapt if needed\n",
        "    # !pip install kaggle  # in Colab, set up API token beforehand\n",
        "    # from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "    # api = KaggleApi(); api.authenticate()\n",
        "    # api.dataset_download_files('tanvirnwu/cat-dog-image-and-video', path='dataset_kaggle', unzip=True)\n",
        "    raise FileNotFoundError(f\"{FRAMES_NPY} not found. Upload or create frames by running CartPole recorder.\")\n",
        "\n",
        "# Normalize to [0,1] float32; convert to shape (T, C, H, W)\n",
        "frames = frames.astype(np.float32) / 255.0\n",
        "# if frames shape is (T,H,W) or (T,H,W,1)\n",
        "if frames.ndim == 3:\n",
        "    frames = frames[..., None]\n",
        "if frames.shape[-1] == 3:\n",
        "    # convert to grayscale\n",
        "    frames_gray = np.dot(frames[...,:3], [0.2989, 0.5870, 0.1140])\n",
        "    frames = frames_gray[..., None]\n",
        "frames = np.transpose(frames, (0, 3, 1, 2))  # -> (T, C, H, W)\n",
        "T_total = frames.shape[0]\n",
        "print(\"Processed frames:\", frames.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "0MJ88ugZw-uv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "f84d7cd3-80e0-424c-eb3f-a63cc466dee4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4129212773.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mseq_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialFrames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4129212773.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, frames_array)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframes_array\u001b[0m  \u001b[0;31m# numpy (T, C, H, W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframes_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0miterate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "# >>> Cell 5: sequential dataset helper (generator)\n",
        "class SequentialFrames:\n",
        "    def __init__(self, frames_array):\n",
        "        self.frames = frames_array  # numpy (T, C, H, W)\n",
        "        self.T = frames_array.shape[0]\n",
        "\n",
        "    def iterate_epochs(self, epoch_count=1):\n",
        "        # yield sequences frame-by-frame (no shuffling)\n",
        "        for e in range(epoch_count):\n",
        "            for t in range(self.T):\n",
        "                # yield single frame as torch tensor (1, C, H, W)\n",
        "                img = torch.tensor(self.frames[t:t+1], dtype=torch.float32, device=DEVICE)\n",
        "                yield t, img\n",
        "\n",
        "seq_dataset = SequentialFrames(frames)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "18d6u-e4xMQS",
        "outputId": "a43f00f2-899d-4923-b7f6-c17befb9dacc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'T_total' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3402843052.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_PREV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Warmup first N_PREV frames: encode/decode without context (or with zeros)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mx_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1,C,H,W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mz_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1, code_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'T_total' is not defined"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "# >>> Cell 6: AE training loop (sequential, with buffer and scheduled sampling)\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=LEARNING_RATE)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "p_tf = 1.0  # teacher forcing probability\n",
        "for epoch in range(1, AE_EPOCHS + 1):\n",
        "    total_loss = 0.0\n",
        "    # scheduled sampling adjustment after TF epochs\n",
        "    use_teacher = epoch <= TEACHER_FORCING_EPOCHS\n",
        "    if not use_teacher:\n",
        "        p_tf = max(0.0, p_tf * SCHEDULED_SAMPLING_DECAY)\n",
        "\n",
        "    # process sequence: maintain buffer of previous z_limited (or z if teacher forcing)\n",
        "    buffer = deque(maxlen=N_PREV)\n",
        "    # Warmup first N_PREV frames: encode/decode without context (or with zeros)\n",
        "    for t in range(T_total):\n",
        "        x_t = torch.tensor(frames[t:t+1], dtype=torch.float32, device=DEVICE)  # (1,C,H,W)\n",
        "        z_t = encoder(x_t)  # (1, code_dim)\n",
        "\n",
        "        # Warmup: if buffer not full, we will decode without prev or with zeros\n",
        "        if len(buffer) < N_PREV:\n",
        "            z_limited = z_t  # full latent during AE training warmup\n",
        "            rec = decoder.decode(z_limited, prev_latents=None)\n",
        "            loss = criterion(rec, x_t)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            # store limited latent (we store full z for teacher-forcing phase)\n",
        "            buffer.append(z_limited.detach())\n",
        "            continue\n",
        "\n",
        "        # now buffer is full (or at least has some items)\n",
        "        # scheduled sampling: decide whether to use GT latents (teacher) or model's past latents:\n",
        "        if use_teacher or (torch.rand(1).item() < p_tf):\n",
        "            prev_stack = torch.cat(list(buffer), dim=0).unsqueeze(0)  # buffer: list of (1,code_dim) -> (n_prev, code_dim)\n",
        "            # reshape to (1, n_prev, code_dim)\n",
        "            prev_stack = prev_stack.view(1, N_PREV, CODE_DIM)\n",
        "        else:\n",
        "            # closed-loop: use previous latents that were produced (we stored them in buffer already)\n",
        "            prev_stack = torch.cat(list(buffer), dim=0).unsqueeze(0).view(1, N_PREV, CODE_DIM)\n",
        "\n",
        "        # Optionally: random K during AE training to make decoder robust to truncation\n",
        "        # We'll randomly pick K among K_CHOICES some fraction of times\n",
        "        if torch.rand(1).item() < 0.3:\n",
        "            K = int(np.random.choice(K_CHOICES))\n",
        "        else:\n",
        "            K = CODE_DIM\n",
        "\n",
        "        z_limited = apply_truncation(z_t, K)\n",
        "\n",
        "        rec = decoder.decode(z_limited, prev_latents=prev_stack)\n",
        "        loss = criterion(rec, x_t)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # push limited latent used for context (detached)\n",
        "        buffer.append(z_limited.detach())\n",
        "\n",
        "    avg_loss = total_loss / T_total\n",
        "    print(f\"AE Epoch {epoch}/{AE_EPOCHS} | avg loss: {avg_loss:.6f} | use_teacher={use_teacher} p_tf={p_tf:.3f}\")\n",
        "\n",
        "# save models\n",
        "torch.save(encoder.state_dict(), \"encoder.pth\")\n",
        "torch.save(decoder.state_dict(), \"decoder.pth\")\n",
        "print(\"Saved encoder/decoder.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elOe4tc20mGE"
      },
      "outputs": [],
      "source": [
        "# >>> Cell 7: quick validation function - greedy K chosen externally or max K\n",
        "def reconstruct_sequence_greedy(encoder, decoder, frames_np, K_choice=128, use_prev_recon=True):\n",
        "    encoder.eval(); decoder.eval()\n",
        "    T = frames_np.shape[0]\n",
        "    recon_frames = []\n",
        "    buffer = deque(maxlen=N_PREV)\n",
        "    with torch.no_grad():\n",
        "        for t in range(T):\n",
        "            x = torch.tensor(frames_np[t:t+1], dtype=torch.float32, device=DEVICE)\n",
        "            z = encoder(x)  # (1, code_dim)\n",
        "            z_l = apply_truncation(z, K_choice)\n",
        "            if len(buffer) < N_PREV:\n",
        "                prev = None\n",
        "            else:\n",
        "                prev = torch.cat(list(buffer), dim=0).unsqueeze(0).view(1, N_PREV, CODE_DIM)\n",
        "            rec = decoder.decode(z_l, prev_latents=prev)  # (1,1,H,W)\n",
        "            recon_frames.append(rec.cpu().numpy()[0,0])  # (H,W)\n",
        "            buffer.append(z_l)  # use z_l as prev latent (closed-loop)\n",
        "    return np.stack(recon_frames, axis=0)\n",
        "\n",
        "# example: reconstruct with K=128\n",
        "recon = reconstruct_sequence_greedy(encoder, decoder, frames, K_choice=128)\n",
        "print(\"recon shape\", recon.shape)\n",
        "# save a short preview as mp4 (normalize to 0..255)\n",
        "sample_vid = (recon[:200] * 255).astype(np.uint8)\n",
        "imageio.mimsave(\"recon_preview_k128.mp4\", sample_vid, fps=30)\n",
        "print(\"Saved recon_preview_k128.mp4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# >>> New Cell: visualize original vs reconstructed\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# –ø–æ–∫–∞–∂–µ–º –ø–µ—Ä–≤—ã–µ 5 –∫–∞–¥—Ä–æ–≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö\n",
        "n_show = 5\n",
        "fig, axes = plt.subplots(2, n_show, figsize=(12, 4))\n",
        "for i in range(n_show):\n",
        "    axes[0, i].imshow(frames[i, 0], cmap='gray')\n",
        "    axes[0, i].set_title(f\"Original {i}\")\n",
        "    axes[0, i].axis('off')\n",
        "    axes[1, i].imshow(recon[i], cmap='gray')\n",
        "    axes[1, i].set_title(f\"Reconstructed {i}\")\n",
        "    axes[1, i].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bKULQyi4CL5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# >>> New Cell: make GIF comparison of original vs reconstructed\n",
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "n_frames = 10  # —Å–∫–æ–ª—å–∫–æ –∫–∞–¥—Ä–æ–≤ –ø–æ–∫–∞–∑–∞—Ç—å\n",
        "frames_orig = (frames[:n_frames, 0] * 255).astype(np.uint8)\n",
        "frames_rec = (recon[:n_frames] * 255).astype(np.uint8)\n",
        "\n",
        "# –°–æ–∑–¥–∞–¥–∏–º side-by-side –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "combined = []\n",
        "for i in range(n_frames):\n",
        "    top = frames_orig[i]\n",
        "    bottom = frames_rec[i]\n",
        "    concat = np.concatenate([top, bottom], axis=1)  # —Å–ª–µ–≤–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª, —Å–ø—Ä–∞–≤–∞ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è\n",
        "    combined.append(concat)\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º GIF\n",
        "imageio.mimsave(\"comparison.gif\", combined, fps=2)\n",
        "print(\"‚úÖ Saved comparison.gif (original vs reconstructed)\")\n",
        "\n",
        "from IPython.display import Image\n",
        "Image(filename=\"comparison.gif\")\n"
      ],
      "metadata": {
        "id": "ME2UKfdup43a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wi-kJa-Mp5MM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}