{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xCetfVXvodm",
        "outputId": "266e97c3-58b0-4964-a6d0-c81f634b8aea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium[classic_control] in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (0.0.4)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (2.6.1)\n"
          ]
        }
      ],
      "source": [
        "pip install gymnasium[classic_control]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install imageio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLRw9ZzzZTbD",
        "outputId": "633ca062-0c4c-48e3-bb04-6508eb51ac31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imageio in /usr/local/lib/python3.12/dist-packages (2.37.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from imageio) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio) (11.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QQauhA8vrAn",
        "outputId": "dd92aba7-6640-4b9a-af0d-0fa668bd01c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# >>> Cell 1: imports + hyperparams\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# hyperparams\n",
        "CODE_DIM = 128\n",
        "N_PREV = 5\n",
        "IMG_H = IMG_W = 64\n",
        "BATCH_SEQ = 1   # we process sequentially per frame; for AE training we'll use batch 1 per time-step\n",
        "AE_EPOCHS = 12\n",
        "TEACHER_FORCING_EPOCHS = 5    # use GT prev latents for these many epochs (teacher forcing)\n",
        "SCHEDULED_SAMPLING_DECAY = 0.95  # multiply p_tf each epoch after TF phase\n",
        "K_CHOICES = [128, 96, 64, 32]   # absolute K's\n",
        "LAMBDA = 0.01  # penalty coefficient for K in reward (tune)\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "# file names (adjust to your paths)\n",
        "FRAMES_NPY = \"cartpole_frames.npy\"  # if using cartpole frames saved earlier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Q62nVJ4pwtOi"
      },
      "outputs": [],
      "source": [
        "# >>> Cell 2: helper utilities (truncation, psnr)\n",
        "import math\n",
        "\n",
        "def apply_truncation(z, K):\n",
        "    \"\"\"\n",
        "    z: (B, code_dim)\n",
        "    K: int (<= code_dim)\n",
        "    returns z_limited with zeros after index K\n",
        "    \"\"\"\n",
        "    if K >= z.shape[1]:\n",
        "        return z\n",
        "    z_l = z.clone()\n",
        "    z_l[:, K:] = 0.0\n",
        "    return z_l\n",
        "\n",
        "def psnr_from_mse(mse, max_val=1.0):\n",
        "    # mse is scalar or numpy\n",
        "    return 10.0 * math.log10(max_val * max_val / (mse + 1e-10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "ww23GyBzw3Iz",
        "outputId": "d66a51e1-bbc0-41ea-b0fe-11ca6142f97e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'encoder'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3885613843.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# >>> Cell 3: import Encoder and Decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCODE_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'encoder'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# >>> Cell 3: import Encoder and Decoder\n",
        "from encoder import Encoder\n",
        "from decoder import Decoder\n",
        "\n",
        "encoder = Encoder(input_ch=1, code_dim=CODE_DIM).to(DEVICE)\n",
        "decoder = Decoder(output_ch=1, code_dim=CODE_DIM, n_prev=N_PREV).to(DEVICE)\n",
        "print(encoder)\n",
        "print(decoder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZWH3C0pw-Po",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b8061fc-e824-4b4e-a356-ab49157a764c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded frames shape: (2000, 64, 64, 1)\n",
            "Processed frames: (2000, 1, 64, 64)\n"
          ]
        }
      ],
      "source": [
        "# >>> Cell 4: load dataset (npy) or Kaggle dataset snippet\n",
        "\n",
        "# If you have a local .npy of frames: load it\n",
        "if os.path.exists(FRAMES_NPY):\n",
        "    frames = np.load(FRAMES_NPY)   # shape expected (T, H, W, C) where C=1 or 3\n",
        "    print(\"Loaded frames shape:\", frames.shape)\n",
        "else:\n",
        "    # Example: how to download Kaggle dataset via 'kaggle' library â€” uncomment and adapt if needed\n",
        "    # !pip install kaggle  # in Colab, set up API token beforehand\n",
        "    # from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "    # api = KaggleApi(); api.authenticate()\n",
        "    # api.dataset_download_files('tanvirnwu/cat-dog-image-and-video', path='dataset_kaggle', unzip=True)\n",
        "    raise FileNotFoundError(f\"{FRAMES_NPY} not found. Upload or create frames by running CartPole recorder.\")\n",
        "\n",
        "# Normalize to [0,1] float32; convert to shape (T, C, H, W)\n",
        "frames = frames.astype(np.float32) / 255.0\n",
        "# if frames shape is (T,H,W) or (T,H,W,1)\n",
        "if frames.ndim == 3:\n",
        "    frames = frames[..., None]\n",
        "if frames.shape[-1] == 3:\n",
        "    # convert to grayscale\n",
        "    frames_gray = np.dot(frames[...,:3], [0.2989, 0.5870, 0.1140])\n",
        "    frames = frames_gray[..., None]\n",
        "frames = np.transpose(frames, (0, 3, 1, 2))  # -> (T, C, H, W)\n",
        "T_total = frames.shape[0]\n",
        "print(\"Processed frames:\", frames.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MJ88ugZw-uv"
      },
      "outputs": [],
      "source": [
        "# >>> Cell 5: sequential dataset helper (generator)\n",
        "class SequentialFrames:\n",
        "    def __init__(self, frames_array):\n",
        "        self.frames = frames_array  # numpy (T, C, H, W)\n",
        "        self.T = frames_array.shape[0]\n",
        "\n",
        "    def iterate_epochs(self, epoch_count=1):\n",
        "        # yield sequences frame-by-frame (no shuffling)\n",
        "        for e in range(epoch_count):\n",
        "            for t in range(self.T):\n",
        "                # yield single frame as torch tensor (1, C, H, W)\n",
        "                img = torch.tensor(self.frames[t:t+1], dtype=torch.float32, device=DEVICE)\n",
        "                yield t, img\n",
        "\n",
        "seq_dataset = SequentialFrames(frames)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "18d6u-e4xMQS",
        "outputId": "d81f4939-7c09-40b7-c400-cde4665d8120"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3053839182.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1,C,H,W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mz_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1, code_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Warmup: if buffer not full, we will decode without prev or with zeros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;31m# (B, 64, h, w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mflat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# (B, flat_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m# (B, code_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
          ]
        }
      ],
      "source": [
        "# >>> Cell 6: AE training loop (sequential, with buffer and scheduled sampling)\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=LEARNING_RATE)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "p_tf = 1.0  # teacher forcing probability\n",
        "for epoch in range(1, AE_EPOCHS + 1):\n",
        "    total_loss = 0.0\n",
        "    # scheduled sampling adjustment after TF epochs\n",
        "    use_teacher = epoch <= TEACHER_FORCING_EPOCHS\n",
        "    if not use_teacher:\n",
        "        p_tf = max(0.0, p_tf * SCHEDULED_SAMPLING_DECAY)\n",
        "\n",
        "    # process sequence: maintain buffer of previous z_limited (or z if teacher forcing)\n",
        "    buffer = deque(maxlen=N_PREV)\n",
        "    # Warmup first N_PREV frames: encode/decode without context (or with zeros)\n",
        "    for t in range(T_total):\n",
        "        x_t = torch.tensor(frames[t:t+1], dtype=torch.float32, device=DEVICE)  # (1,C,H,W)\n",
        "        z_t = encoder.encode(x_t)  # (1, code_dim)\n",
        "\n",
        "        # Warmup: if buffer not full, we will decode without prev or with zeros\n",
        "        if len(buffer) < N_PREV:\n",
        "            z_limited = z_t  # full latent during AE training warmup\n",
        "            rec = decoder.decode(z_limited, prev_latents=None)\n",
        "            loss = criterion(rec, x_t)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            # store limited latent (we store full z for teacher-forcing phase)\n",
        "            buffer.append(z_limited.detach())\n",
        "            continue\n",
        "\n",
        "        # now buffer is full (or at least has some items)\n",
        "        # scheduled sampling: decide whether to use GT latents (teacher) or model's past latents:\n",
        "        if use_teacher or (torch.rand(1).item() < p_tf):\n",
        "            prev_stack = torch.cat(list(buffer), dim=0).unsqueeze(0)  # buffer: list of (1,code_dim) -> (n_prev, code_dim)\n",
        "            # reshape to (1, n_prev, code_dim)\n",
        "            prev_stack = prev_stack.view(1, N_PREV, CODE_DIM)\n",
        "        else:\n",
        "            # closed-loop: use previous latents that were produced (we stored them in buffer already)\n",
        "            prev_stack = torch.cat(list(buffer), dim=0).unsqueeze(0).view(1, N_PREV, CODE_DIM)\n",
        "\n",
        "        # Optionally: random K during AE training to make decoder robust to truncation\n",
        "        # We'll randomly pick K among K_CHOICES some fraction of times\n",
        "        if torch.rand(1).item() < 0.3:\n",
        "            K = int(np.random.choice(K_CHOICES))\n",
        "        else:\n",
        "            K = CODE_DIM\n",
        "\n",
        "        z_limited = apply_truncation(z_t, K)\n",
        "\n",
        "        rec = decoder.decode(z_limited, prev_latents=prev_stack)\n",
        "        loss = criterion(rec, x_t)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # push limited latent used for context (detached)\n",
        "        buffer.append(z_limited.detach())\n",
        "\n",
        "    avg_loss = total_loss / T_total\n",
        "    print(f\"AE Epoch {epoch}/{AE_EPOCHS} | avg loss: {avg_loss:.6f} | use_teacher={use_teacher} p_tf={p_tf:.3f}\")\n",
        "\n",
        "# save models\n",
        "torch.save(encoder.state_dict(), \"encoder.pth\")\n",
        "torch.save(decoder.state_dict(), \"decoder.pth\")\n",
        "print(\"Saved encoder/decoder.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elOe4tc20mGE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "eda21652-587c-493d-bcb4-332bcacead36"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3761353025.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# example: reconstruct with K=128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mrecon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreconstruct_sequence_greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK_choice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recon shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# save a short preview as mp4 (normalize to 0..255)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3761353025.py\u001b[0m in \u001b[0;36mreconstruct_sequence_greedy\u001b[0;34m(encoder, decoder, frames_np, K_choice, use_prev_recon)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes_np\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1, code_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mz_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_truncation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK_choice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mN_PREV\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;31m# (B, 64, h, w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mflat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# (B, flat_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m# (B, code_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
          ]
        }
      ],
      "source": [
        "# >>> Cell 7: quick validation function - greedy K chosen externally or max K\n",
        "def reconstruct_sequence_greedy(encoder, decoder, frames_np, K_choice=128, use_prev_recon=True):\n",
        "    encoder.eval(); decoder.eval()\n",
        "    T = frames_np.shape[0]\n",
        "    recon_frames = []\n",
        "    buffer = deque(maxlen=N_PREV)\n",
        "    with torch.no_grad():\n",
        "        for t in range(T):\n",
        "            x = torch.tensor(frames_np[t:t+1], dtype=torch.float32, device=DEVICE)\n",
        "            z = encoder.encode(x)  # (1, code_dim)\n",
        "            z_l = apply_truncation(z, K_choice)\n",
        "            if len(buffer) < N_PREV:\n",
        "                prev = None\n",
        "            else:\n",
        "                prev = torch.cat(list(buffer), dim=0).unsqueeze(0).view(1, N_PREV, CODE_DIM)\n",
        "            rec = decoder.decode(z_l, prev_latents=prev)  # (1,1,H,W)\n",
        "            recon_frames.append(rec.cpu().numpy()[0,0])  # (H,W)\n",
        "            buffer.append(z_l)  # use z_l as prev latent (closed-loop)\n",
        "    return np.stack(recon_frames, axis=0)\n",
        "\n",
        "# example: reconstruct with K=128\n",
        "recon = reconstruct_sequence_greedy(encoder, decoder, frames, K_choice=128)\n",
        "print(\"recon shape\", recon.shape)\n",
        "# save a short preview as mp4 (normalize to 0..255)\n",
        "sample_vid = (recon[:200] * 255).astype(np.uint8)\n",
        "imageio.mimsave(\"recon_preview_k128.mp4\", sample_vid, fps=30)\n",
        "print(\"Saved recon_preview_k128.mp4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bKULQyi4CL5_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}